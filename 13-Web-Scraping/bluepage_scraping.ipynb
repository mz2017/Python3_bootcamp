{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping IBM W3 Bluepages\n",
    "\n",
    "This Notebook is an exercise to scrape IBM Bluepage site for employee hierarchy data. This is purely an exercise for learning Python web scraping capability. The result should not be share with anyone outside IBM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "The requirement is to start anywhere in the IBM Bluepage (`BASE_URL`), from the bluepage for any employee), go over all employees reporting to this person, and recursively all their reportees, up to a given levels (`MAX_LEVEL_down`) down. \n",
    "\n",
    "The results will be saved in a CSV file, with following columns:\n",
    "  1. Serial ID\n",
    "  2. Employee full name\n",
    "  3. Employee email address\n",
    "  4. Employee location\n",
    "  5. Manager's ID\n",
    "  6. Number of direct reports\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages used\n",
    "A common web scraping application uses requests and BeautifulSoup packages. That works well for source file in HTML. If a web page is dynamically rendered on client side using JavaScript, such as IBM Bluepage, you won't be able to use the same approach, as the requests package will only get you the raw source in JavaScript, not the final HTML content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this challenge, we need to use additional help. There are many options out there, such as dryscrape. But I was only able to get it work on my Wondows 10 computer using selenium. You need to install Python selenium pacjkge first: `pip install selenium`. But there is another step required to get the Chrome driver installed manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the instructions to setup the Chrome engine for selenium. First, download the ChromeDriver from this [web site](https://sites.google.com/a/chromium.org/chromedriver/downloads). In my case, I [downloaded the win32 version](https://chromedriver.storage.googleapis.com/index.html?path=90.0.4430.24/) to my Windows machine, unzipped it to a folder, and add it to my system `PATH` environment variable. That is all I had to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to go. Will start by importing all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup some basic configurations for the scraping, such as starting point, and the depth to traverse. The `max_level_down` below means we will go at most 2 levels below the base_URL, as base_url represents level 0, one level down is considered to be level 1, and so on.\n",
    "\n",
    "The following code block contains all the configurations which can be applied to the scraping process. Don't change any other code unless you want to make some processing logic change, or bug fixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with Arvind's page, change this value if want to start from someone else\n",
    "base_url = 'https://w3.ibm.com/bluepages/profile.html?uid=067308897'\n",
    "\n",
    "# How many levels down from above page\n",
    "max_level_down = 2\n",
    "\n",
    "# Set start level to default value 0\n",
    "start_level = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a queue for traversing the tree of the employees. Each entry in the queue is a tuple with employ page URL and their level. Start the queue with one record, with value of `(base_url,0)`. We will append all reportees to the queue for each record we process, until we hit the `max_level`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to do the scraping. Will start with a function to scrape one page, and get the user information out from this page, and add the reportee info to the queue. The input to the function is simply a record in the `process_queue`, and the return is tuple of all the required information from this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeOnePage(pageInfo):\n",
    "    url = pageInfo[0]\n",
    "    level = pageInfo[1]\n",
    "    driver = webdriver.Chrome()\n",
    "    soup = getSoupForUrl(driver, url)\n",
    "    # print(soup)\n",
    "    info = scrapeUserInfo(soup)\n",
    "    \n",
    "    # close the chrome page for the current URL\n",
    "    driver.quit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSoupForUrl(driver, url):\n",
    "    driver.get(url)\n",
    "    \n",
    "    # the following line is unique to the bluepage. If scraping another web site, \n",
    "    # you need to find something matches your site design.\n",
    "    element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"profile-w3-section\"))) \n",
    "    #waits 10 seconds until element is located. Can have other wait conditions  such as visibility_of_element_located or text_to_be_present_in_element\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = bs(html, \"lxml\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeUserInfo(soup):\n",
    "    name = getName(soup)\n",
    "    serial = getSerial(soup)\n",
    "    title = getTitle(soup)\n",
    "    email = getEmail(soup, serial)\n",
    "    address = getLocation(soup)\n",
    "    reports = getNumReports(soup)\n",
    "    manager = getManagerId(soup)\n",
    "    return [serial, name, title, email, address, reports, manager]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above are main processing functions. The following are functions for each data fields to be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getName(soup):\n",
    "    name = soup.select('.name-link.loader-anim.field-text')[0].text\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmail(soup, serial):\n",
    "    idValue = '#copy-icon-header-'+serial\n",
    "    email = soup.select(idValue)[0].text\n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSerial(soup):\n",
    "    serial = soup.select('#serial-number')[0].text\n",
    "    values = serial.split(':')\n",
    "    serial = values[1].strip()\n",
    "    return serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocation(soup):\n",
    "    address = soup.select('.location-link-metrics')[0].text\n",
    "    return address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitle(soup):\n",
    "    title = soup.select('.card-h3.ellipsis.hyphenate.loader-anim.active')[1].span.text\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumReports(soup):\n",
    "    numReports = soup.select('.team-full-link')\n",
    "    n = 0\n",
    "    if len(numReports) > 0:\n",
    "        raw = numReports[0].text\n",
    "        n = extractValueInParenthesis(raw)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getManagerId(soup):\n",
    "    res = soup.select('.report-name.pd-left-65')\n",
    "    mid = 'null'\n",
    "    if len(res) > 0 :\n",
    "        mid = res[0].a['data-teammember-uid']\n",
    "    return mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDirectReportIds(soup):\n",
    "    persons = soup.select(\"#anchorReports\")\n",
    "    ids = []\n",
    "    if persons:\n",
    "        for p in persons[0].find_all('li', {'class':'person'}):\n",
    "            # name = p.find('span', {'class':'person-name'}).text\n",
    "            pid = p.div['data-teammember-uid']\n",
    "            ids.append(pid)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractValueInParenthesis(s):\n",
    "    result = re.search(r\"\\(([A-Za-z0-9_]+)\\)\", s)\n",
    "    return result.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOutputFileName():\n",
    "    now = datetime.now()\n",
    "    timestring = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    return 'bluepage-'+timestring+'.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all functions defined, we can now start the processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"location-link-metrics\" href=\"http://w3.ibm.com/workplaces/location.html?campusid=HQ1ARM1\" role=\"link\" tabindex=\"2\" target=\"_blank\">Armonk, NY , United States</a>, <a class=\"location-link-metrics\" href=\"http://w3.ibm.com/workplaces/location.html?campusid=HQ1ARM1\" role=\"link\" tabindex=\"2\" target=\"_blank\">Armonk, NY , United States</a>]\n"
     ]
    }
   ],
   "source": [
    "# this is the main function for the scraping\n",
    "\n",
    "# initialize the processing queue. Each record in the queue is a tuple with URL and Level.\n",
    "process_queue = deque([(base_url, start_level)])\n",
    "\n",
    "# setup the CSV export file\n",
    "filename = getOutputFileName()\n",
    "output_file = open(filename,'w',newline='')\n",
    "csv_writer = csv.writer(output_file,delimiter=',')\n",
    "csv_writer.writerow(['Serial','Name','Title', 'email', 'Address', '#reports', 'Manager'])\n",
    "\n",
    "# Start the total record count:\n",
    "total_count = 0\n",
    "\n",
    "while len(process_queue) > 0:\n",
    "    record = process_queue.popleft()\n",
    "    # process user info for the current record, save data to CSV\n",
    "    # the same function will also add any child records to the processing queue\n",
    "    scrapeOnePage(record)\n",
    "    \n",
    "# close the csv file\n",
    "output_file.close()\n",
    "print('Scraped a total number of {} records to file {}'.format(total_count, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are test scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "scrapeOnePage(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BluePages'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.get(base_url)\n",
    "soup = bs4.BeautifulSoup(res.text,\"lxml\")\n",
    "soup.select('title')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "# url = 'https://w3.ibm.com/bluepages/profile.html?uid=1G4482897'  # ming\n",
    "url = 'https://w3.ibm.com/bluepages/profile.html?uid=067308897'  # Arvind \n",
    "# url = 'https://w3.ibm.com/bluepages/profile.html?uid=934567897'  # Rob\n",
    "driver.get(url)\n",
    "currentLevel = 6\n",
    "\n",
    "element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'profile-w3-section'))) \n",
    "\n",
    "#if currentLevel > 0:\n",
    "#    element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"anchorNonPersonReports\"))) \n",
    "#else:\n",
    "#    element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"anchorReports\"))) \n",
    "#waits 10 seconds until element is located. Can have other wait conditions  such as visibility_of_element_located or text_to_be_present_in_element\n",
    "\n",
    "html = driver.page_source\n",
    "soup = bs(html, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['067308897',\n",
       " 'Arvind Krishna',\n",
       " 'Chairman and Chief Executive Officer',\n",
       " 'arvindk@us.ibm.com',\n",
       " 'Armonk, NY , United States',\n",
       " '16',\n",
       " 'null']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = getName(soup)\n",
    "serial = getSerial(soup)\n",
    "title = getTitle(soup)\n",
    "email = getEmail(soup, serial)\n",
    "address = getLocation(soup)\n",
    "reports = getNumReports(soup)\n",
    "manager = getManagerId(soup)\n",
    "[serial, name, title, email, address, reports, manager]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Armonk, NY , United States\n"
     ]
    }
   ],
   "source": [
    "address = soup.select('.location-link-metrics')[0].text\n",
    "print(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arvind Krishna\n"
     ]
    }
   ],
   "source": [
    "name = soup.select('.name-link.loader-anim.field-text')[0].text\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chairman and Chief Executive Officer'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = soup.select('.card-h3.ellipsis.hyphenate.loader-anim.active')[1].span.text\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "067308897\n"
     ]
    }
   ],
   "source": [
    "serial = soup.select('#serial-number')[0].text\n",
    "values = serial.split(':')\n",
    "serial = values[1].strip()\n",
    "print(serial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reports (16)\n",
      "<re.Match object; span=(8, 12), match='(16)'>\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "numReports = soup.select('.team-full-link')\n",
    "if len(numReports) > 0:\n",
    "    raw = numReports[0].text\n",
    "    print(raw)\n",
    "    n = extractValueInParenthesis(raw)\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "n = getNumReports(soup)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Jonathan Adashek,  id: 4J6316897\n",
      "Name: Howard Boville,  id: 4J8426897\n",
      "Name: Michelle Browdy,  id: 5D0691897\n",
      "Name: Gary Cohn,  id: 5J2912897\n",
      "Name: Mark Foster,  id: 6G1701897\n",
      "Name: Darío Gil ,  id: 784626897\n",
      "Name: James Kavanaugh,  id: 914589897\n",
      "Name: John Kelly,  id: 086767897\n",
      "Name: Nickle LaMoreaux,  id: 3A1811897\n",
      "Name: Carla Piñeyro Sublett,  id: 5J3109897\n",
      "Name: Tom Rosamilia,  id: 579070897\n",
      "Name: Martin Schroeter,  id: 216989897\n",
      "Name: Martin Schroeter,  id: 6J2643897\n",
      "Name: Bridget Van Kralingen,  id: 9A7664897\n",
      "Name: Jim Whitehurst,  id: 3J7003897\n",
      "Name: Kate Woolley,  id: 4J8390897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['4J6316897',\n",
       " '4J8426897',\n",
       " '5D0691897',\n",
       " '5J2912897',\n",
       " '6G1701897',\n",
       " '784626897',\n",
       " '914589897',\n",
       " '086767897',\n",
       " '3A1811897',\n",
       " '5J3109897',\n",
       " '579070897',\n",
       " '216989897',\n",
       " '6J2643897',\n",
       " '9A7664897',\n",
       " '3J7003897',\n",
       " '4J8390897']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test direct reports\n",
    "persons = soup.select(\"#anchorReports\")\n",
    "ids = []\n",
    "if persons:\n",
    "    for p in persons[0].find_all('li', {'class':'person'}):\n",
    "        name = p.find('span', {'class':'person-name'}).text\n",
    "        pid = p.div['data-teammember-uid']\n",
    "        ids.append(pid)\n",
    "        print('Name: {},  id: {}'.format(name, pid))\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3J7003897\n"
     ]
    }
   ],
   "source": [
    "# test manager ID\n",
    "res = soup.select('.report-name.pd-left-65')\n",
    "mid = 'null'\n",
    "res\n",
    "if len(res) > 0 :\n",
    "    mid = res[0].a['data-teammember-uid']\n",
    "print(mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test CSV export\n",
    "filename = getOutputFileName()\n",
    "output_file = open(filename,'w',newline='')\n",
    "csv_writer = csv.writer(output_file,delimiter=',')\n",
    "csv_writer.writerow(['Serial','Name','Title', 'email', 'Address', '#reports', 'Manager'])\n",
    "uinfo = scrapeUserInfo(soup)\n",
    "csv_writer.writerow(uinfo)\n",
    "output_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOutputFileName():\n",
    "    now = datetime.now()\n",
    "    timestring = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    return 'bluepage-'+timestring+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
